# Web Scraping Template with Playwright

Este es un **scraper web** personalizable construido con [Python](https://www.python.org/) y [Playwright](https://playwright.dev/). DiseÃ±ado para extraer datos de sitios web con paginaciÃ³n, preservando el estado del progreso y exportando los resultados a CSV de manera incremental.

## ğŸš€ CaracterÃ­sticas

- **Persistencia de Estado**: Guarda automÃ¡ticamente el progreso (pÃ¡gina actual y URLs procesadas) en `scraping_state.json`. Si el proceso se interrumpe, puedes reanudarlo sin duplicar trabajo.
- **ExportaciÃ³n a CSV**: Los datos extraÃ­dos se guardan en tiempo real en `scraping_results.csv`.
- **OptimizaciÃ³n de Rendimiento**: Bloquea la carga de imÃ¡genes para acelerar la navegaciÃ³n y reducir el consumo de ancho de banda.
- **Manejo de Errores**: 
  - Detecta y maneja errores HTTP (404, 500).
  - Evita reprocesar URLs ya visitadas.
  - Cierra pestaÃ±as secundarias automÃ¡ticamente, incluso si ocurre un error.
- **NavegaciÃ³n por PaginaciÃ³n**: LÃ³gica integrada para navegar a travÃ©s de mÃºltiples pÃ¡ginas de resultados.
- **Logging con Iconos**: Sistema de logging visual con emojis para fÃ¡cil seguimiento del progreso.
- **Seguimiento de Tiempo**: Muestra el tiempo de ejecuciÃ³n al finalizar o en caso de error.

## ğŸ“‹ Requisitos Previos

-  Creado con Python 3.12
- [uv](https://github.com/astral-sh/uv) (Opcional, pero recomendado para gestiÃ³n de dependencias)

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio o descargar los archivos.**

2. **Instalar dependencias:**

   Si usas `uv`:
   ```bash
   uv sync
   ```

   O con `pip` tradicional:
   ```bash
   pip install playwright
   playwright install chromium
   ```

## âš™ï¸ ConfiguraciÃ³n

El archivo `main.py` estÃ¡ organizado en secciones claramente definidas. AquÃ­ estÃ¡n los elementos que **debes configurar**:

### 1. Constantes de ConfiguraciÃ³n (lÃ­neas 18-34)

```python
# =============================================================================
# CONFIGURACIÃ“N
# =============================================================================
BASE_URL = "https://www.example.com"
TARGET_URL = "https://www.example.com/page=1"
PAGE_LOAD_TIMEOUT = 3000  # ms
ELEMENT_LOAD_TIMEOUT = 1500  # ms
INTER_REQUEST_DELAY = 500  # ms

# Encabezados de columnas CSV
CSV_HEADERS = [["TÃ­tulo", "Fecha", "Document_URL"]]

# ConfiguraciÃ³n del navegador
BROWSER_CONFIG = {
    "headless": False,
    # "executable_path": r"C:\ruta\a\navegador.exe",  # Opcional
}
```

| Variable | DescripciÃ³n |
|----------|-------------|
| `BASE_URL` | URL base del sitio (para construir URLs relativas) |
| `TARGET_URL` | URL inicial con paginaciÃ³n (ej: `page=1`) |
| `PAGE_LOAD_TIMEOUT` | Tiempo de espera despuÃ©s de cargar pÃ¡gina principal (ms) |
| `ELEMENT_LOAD_TIMEOUT` | Tiempo de espera despuÃ©s de cargar pÃ¡gina de elemento (ms) |
| `INTER_REQUEST_DELAY` | Delay entre peticiones para evitar bloqueos (ms) |
| `CSV_HEADERS` | Encabezados de las columnas del CSV de salida |
| `BROWSER_CONFIG` | ConfiguraciÃ³n de Playwright (headless, executable_path, etc.) |

### 2. Selectores (busca `TODO:`)

Debes completar los selectores de Playwright en estas lÃ­neas:

| LÃ­nea | PropÃ³sito | Ejemplo |
|-------|-----------|---------|
| 170 | Elementos de lista a procesar | `page.locator('a.item-link').all()` |
| 205 | URL del documento | `element_page.locator('a.download-btn')` |
| 206 | TÃ­tulo del elemento | `element_page.locator('h1.title')` |
| 207 | Fecha del elemento | `element_page.locator('span.date')` |
| 252 | InformaciÃ³n de paginaciÃ³n | `page.locator('span.page-info')` |

### 3. Alternativas Comentadas

El cÃ³digo incluye alternativas comentadas para diferentes escenarios:

- **PaginaciÃ³n por URL vs botÃ³n** (lÃ­neas 152-155, 259-266)
- **URLs relativas vs absolutas** (lÃ­nea 175)
- **Abrir pestaÃ±as con middle-click** (lÃ­neas 191-195)

## ğŸ“‚ Estructura del Proyecto

```
Web Scraping/
â”œâ”€â”€ main.py              # LÃ³gica principal del scraper
â”œâ”€â”€ progress_state.py    # MÃ³dulo de gestiÃ³n de estado y CSV
â”œâ”€â”€ scraping_results.csv # Archivo de salida (auto-generado)
â”œâ”€â”€ scraping_state.json  # Estado de progreso (auto-generado)
â”œâ”€â”€ pyproject.toml       # Dependencias del proyecto
â””â”€â”€ README.md            # Este archivo
```

### Estructura de `main.py`

El archivo estÃ¡ organizado en secciones:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONFIGURACIÃ“N                       â”‚  â† Constantes y configuraciÃ³n
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FUNCIONES UTILITARIAS               â”‚  
â”‚  â€¢ create_route_interceptor()       â”‚  â† Bloquea imÃ¡genes
â”‚  â€¢ create_new_page()                â”‚  â† Crea pÃ¡ginas con interceptor
â”‚  â€¢ log_progress()                   â”‚  â† Logging con iconos
â”‚  â€¢ log_time()                       â”‚  â† Registra tiempo de ejecuciÃ³n
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LÃ“GICA PRINCIPAL DE SCRAPING        â”‚
â”‚  â€¢ run()                            â”‚  â† FunciÃ³n principal
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PUNTO DE ENTRADA                    â”‚
â”‚  â€¢ if __name__ == "__main__"        â”‚  â† EjecuciÃ³n del script
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estructura de `progress_state.py`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONFIGURACIÃ“N                       â”‚
â”‚  â€¢ STATE_FILE, CSV_FILE             â”‚  â† Nombres de archivos
â”‚  â€¢ DEFAULT_STATE                    â”‚  â† Estado inicial
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GESTIÃ“N DE ESTADO                   â”‚
â”‚  â€¢ load_state()                     â”‚  â† Carga estado previo
â”‚  â€¢ save_state()                     â”‚  â† Guarda estado actual
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GESTIÃ“N DE CSV                      â”‚
â”‚  â€¢ save_to_csv_init()               â”‚  â† Inicializa CSV con headers
â”‚  â€¢ save_to_csv()                    â”‚  â† AÃ±ade filas al CSV
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## â–¶ï¸ Uso

Ejecuta el script principal:

Si usas `uv`:
```bash
uv run main.py
```

Si usas `pip`:
```bash
python main.py
```

### Salida en Consola

El scraper muestra progreso visual con emojis:

```
ğŸ“Œ Tiempo de inicio: 2024-01-09 10:30:00
ğŸ“Œ Iniciando desde la pÃ¡gina 1
ğŸ“Š URLs ya procesadas: 0
ğŸ“Š Documentos ya procesados: 0
ğŸ“„ Procesando pÃ¡gina 1 (25 elementos)
âœ… [1/25] ['TÃ­tulo del artÃ­culo', '2024-01-01', 'https://...']
â­ï¸  [2/25] Ya procesada, saltando...
âš ï¸  [3/25] No se encontrÃ³ url
ğŸ’¾ Guardados 23 registros de la pÃ¡gina 1
ğŸ“ PÃ¡gina 1 de 10
...
âœ… Â¡Scraping completado!
â±ï¸  runtime: 0:15:32.456789
ğŸ“Š Total de registros extraÃ­dos: 250
```

## ï¿½ Funciones Utilitarias

### `log_progress(message, level)`

Sistema de logging con niveles e iconos:

| Level | Icono | Uso |
|-------|-------|-----|
| `info` | ğŸ“„ | InformaciÃ³n general |
| `success` | âœ… | OperaciÃ³n exitosa |
| `warning` | âš ï¸ | Advertencias |
| `error` | âŒ | Errores |
| `skip` | â­ï¸ | Elementos saltados |
| `save` | ğŸ’¾ | Datos guardados |
| `start` | ğŸ“Œ | Inicio de proceso |
| `stats` | ğŸ“Š | EstadÃ­sticas |
| `nav` | â© | NavegaciÃ³n |
| `page` | ğŸ“ | InformaciÃ³n de pÃ¡gina |
| `time` | â±ï¸ | Tiempo de ejecuciÃ³n |

### `log_time(start_time)`

Calcula y muestra el tiempo transcurrido desde `start_time`.

### `create_new_page(browser, route_interceptor)`

Crea una nueva pÃ¡gina con el interceptor de imÃ¡genes configurado.

## âš ï¸ Notas Importantes

- Este script estÃ¡ configurado con `headless=False` para que veas el navegador trabajar.
- Puedes especificar un navegador diferente (Chrome, Brave, Edge) usando `executable_path` en `BROWSER_CONFIG`.
- AsegÃºrate de respetar los tÃ©rminos de servicio (ToS) y el archivo `robots.txt` del sitio web que estÃ¡s scrapeando.
- El cÃ³digo incluye type hints para mejor mantenibilidad y autocompletado en IDEs.

## ğŸ“ Licencia

Este proyecto es una plantilla de uso libre. ModifÃ­calo segÃºn tus necesidades.
