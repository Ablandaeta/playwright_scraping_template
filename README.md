# Web Scraping Template with Playwright

Este es un **scraper web** personalizable construido con [Python](https://www.python.org/) y [Playwright](https://playwright.dev/). Dise√±ado para extraer datos de sitios web con paginaci√≥n, preservando el estado del progreso y exportando los resultados a CSV de manera incremental.

## üöÄ Caracter√≠sticas

- **Persistencia de Estado**: Guarda autom√°ticamente el progreso (p√°gina actual y URLs procesadas) en `scraping_state.json`. Si el proceso se interrumpe, puedes reanudarlo sin duplicar trabajo.
- **Exportaci√≥n a CSV**: Los datos extra√≠dos se guardan en tiempo real en `scraping_results.csv`.
- **Optimizaci√≥n de Rendimiento**: Bloquea la carga de im√°genes para acelerar la navegaci√≥n y reducir el consumo de ancho de banda.
- **Manejo de Errores**: 
  - Detecta y maneja errores HTTP (404, 500).
  - Evita reprocesar URLs ya visitadas.
  - Cierra pesta√±as secundarias autom√°ticamente, incluso si ocurre un error.
- **Navegaci√≥n por Paginaci√≥n**: L√≥gica integrada para navegar a trav√©s de m√∫ltiples p√°ginas de resultados.

## üìã Requisitos Previos

-  Creado con Python 3.12
- [uv](https://github.com/astral-sh/uv) (Opcional, pero recomendado para gesti√≥n de dependencias)

## üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio o descargar los archivos.**

2. **Instalar dependencias:**

   Si usas `uv`:
   ```bash
   uv sync
   ```

   O con `pip` tradicional:
   ```bash
   pip install playwright
   playwright install chromium
   ```

## ‚öôÔ∏è Configuraci√≥n

El archivo `main.py` contiene marcadores de posici√≥n que **debes configurar** para adaptarlo a tu objetivo de scraping:

1. **URL Objetivo**: 
   Edita la variable `url` en la l√≠nea 12 con la direcci√≥n web inicial.
   ```python
   url = "https://ejemplo.com/lista-items"
   ```

2. **Selectores**:
   Busca los comentarios `# Completar con el selector adecuado` y llena los m√©todos `locator()` con los atributos de los elementos de la p√°gina objetivo:
   - **Elementos de lista** (l√≠nea 48): El contenedor de cada √≠tem a extraer.
   - **T√≠tulo/Datos** (l√≠nea 65): El dato espec√≠fico dentro de la p√°gina de detalle.
   - **Paginaci√≥n** (l√≠neas 103, 114): Selectores para el n√∫mero de p√°gina actual/total y el bot√≥n "Siguiente".

3. **Base URL** (Opcional):
   Si los enlaces extra√≠dos son relativos (ej: `/item/1`), configura `base_url` en la l√≠nea 53.

4. **Personaliza para tu necesidad**:
   Esta plantilla es una estructura minimalista, puedes personalizarla para adaptarla a tus necesidades y a tu p√°gina objetivo ya que no todos los sitios web tienen la misma estructura y selectores. Puedes agregar m√°s selectores, funciones, pesta√±as, etc. 

## ‚ñ∂Ô∏è Uso

Ejecuta el script principal:

Si usas `uv`:
   ```bash
   uv run main.py
   ```

Si usas `pip`:
```bash
python main.py
```

- El scraper abrir√° un navegador Chromium (visible por defecto para depuraci√≥n).
- Ver√°s el progreso en la consola.
- Los datos se guardar√°n en `scraping_results.csv`.
- El estado (para reanudar) se guarda en `scraping_state.json`.

## üìÇ Estructura del Proyecto

- `main.py`: L√≥gica principal del scraper, flujo de navegaci√≥n y extracci√≥n.
- `progress_state.py`: M√≥dulos auxiliares para cargar/guardar el estado JSON y manejar la escritura CSV.
- `scraping_results.csv`: Archivo de salida (se genera autom√°ticamente).
- `scraping_state.json`: Archivo de control de progreso (se genera autom√°ticamente).

## ‚ö†Ô∏è Notas Importantes

- Este script est√° configurado con `headless=False` (l√≠nea 14) para que veas el navegador trabajar. Para producci√≥n o mayor velocidad, c√°mbialo a `headless=True`.
- Aseg√∫rate de respetar los t√©rminos de servicio (ToS) y el archivo `robots.txt` del sitio web que est√°s scrapeando.
